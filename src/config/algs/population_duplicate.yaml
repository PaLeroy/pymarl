# --- population specific parameters ---
# --- This config file uses the episodic runner, which is useful for testing locally ---

# Global parameter, indep of agent type
runner_function: "population"
runner: "episode_runner_population"
batch_size_run: 1 # Number of env at the same time

name: "population_2_2"


use_cuda: True

matchmaking: "random"

save_model_interval: 20000
save_model: True # Save the models to disk

use_tensorboard: True

n_agent_type: 2

log_interval: 20
runner_log_interval: 20
learner_log_interval: 20
agent_type_1:
  number: 2 # number of agent of this type in the population
  mac: "basic_mac"
  action_selector: "epsilon_greedy"
  epsilon_start: 1.0
  epsilon_finish: 0.05
  epsilon_anneal_time: 500000

  gamma: 0.99

  buffer_size: 5000

  target_update_interval: 200

  agent_output_type: "q"
  learner: "q_learner"
  double_q: True
  mixer: "qmix"
  mixing_embed_dim: 32

  learner_log_interval: 2000 # Log training stats every {} timesteps


  # --- RL hyperparameters ---
  batch_size: 32 # Number of episodes to train on
  lr: 0.0005 # Learning rate for agents
  critic_lr: 0.0005 # Learning rate for critics
  optim_alpha: 0.99 # RMSProp alpha
  optim_eps: 0.00001 # RMSProp epsilon
  grad_norm_clip: 10 # Reduce magnitude of gradients above this L2 norm

  # --- Agent parameters ---
  agent: "rnn" # Default rnn agent
  rnn_hidden_dim: 64 # Size of hidden state for default rnn agent
  obs_agent_id: True # Include the agent's one_hot id in the observation
  obs_last_action: True # Include the agent's last action (one_hot) in the observation

  # --- Logging options ---
  save_model: [True, True] # Save the models to disk
  save_model_interval: [200000, 200000] # Save models after this many timesteps
  checkpoint_path: ["", ""] # Load a checkpoint from this path
  load_step: [0, 0] # Load model trained on this many timesteps (0 if choose max possible)

agent_type_2:
  number: 2
  mac: "basic_mac"
  action_selector: "multinomial"
  epsilon_start: .5
  epsilon_finish: .01
  epsilon_anneal_time: 100000
  mask_before_softmax: False

  gamma: 0.99

  buffer_size: 8

  # update the target network every {} training steps
  target_update_interval: 200

  lr: 0.0005
  critic_lr: 0.0005
  td_lambda: 0.8

  # use COMA
  agent_output_type: "pi_logits"
  learner: "coma_learner"
  critic_q_fn: "coma"
  critic_baseline_fn: "coma"
  critic_train_mode: "seq"
  critic_train_reps: 1
  q_nstep: 0  # 0 corresponds to default Q, 1 is r + gamma*Q, etc

  learner_log_interval: 2000 # Log training stats every {} timesteps


  # --- RL hyperparameters ---
  batch_size: 32 # Number of episodes to train on
  optim_alpha: 0.99 # RMSProp alpha
  optim_eps: 0.00001 # RMSProp epsilon
  grad_norm_clip: 10 # Reduce magnitude of gradients above this L2 norm

  # --- Agent parameters ---
  agent: "rnn" # Default rnn agent
  rnn_hidden_dim: 64 # Size of hidden state for default rnn agent
  obs_agent_id: True # Include the agent's one_hot id in the observation
  obs_last_action: True # Include the agent's last action (one_hot) in the observation

  # --- Logging options ---
  save_model: [True, True] # Save the models to disk
  save_model_interval: [200000, 200000] # Save models after this many timesteps
  checkpoint_path: ["", ""] # Load a checkpoint from this path
  load_step: [0, 0] # Load model trained on this many timesteps (0 if choose max possible)




